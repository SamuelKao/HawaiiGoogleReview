{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from gensim.models import Word2Vec\n",
    "import dateutil\n",
    "from scipy.sparse import lil_matrix # To build sparse feature matrices, if you like\n",
    "import requests\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if to download datafile to local\n",
    "# URL of the gzipped JSON file\n",
    "url = \"https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/review-Hawaii_10.json.gz\"\n",
    "\n",
    "# Define a local file to save the gzipped content\n",
    "local_file = \"review-Hawaii.json.gz\"\n",
    "\n",
    "# Download the file in chunks\n",
    "with requests.get(url, stream=True) as response:\n",
    "    response.raise_for_status()  # Raise an error if the download fails\n",
    "    with open(local_file, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):  # Adjust chunk size as needed\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompress and load the JSON data\n",
    "local_file = \"review-Hawaii.json.gz\"\n",
    "dataset_review_Hawaii = []\n",
    "with gzip.open(local_file, \"rt\", encoding=\"utf-8\") as f:  # \"rt\" mode for text\n",
    "    for line in f:\n",
    "        data = json.loads(line)  # Parse each JSON object\n",
    "        dataset_review_Hawaii.append(data)\n",
    "\n",
    "# Output the length of the dataset to verify\n",
    "print(f\"Loaded {len(dataset_review_Hawaii)} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_Hawaii_clean = [i for i in dataset_review_Hawaii if i['text'] != None]\n",
    "review_Hawaii_clean = pd.DataFrame(review_Hawaii_clean)\n",
    "review_Hawaii_clean_eng = review_Hawaii_clean[review_Hawaii_clean['text'].str.match(r'\\w')]\n",
    "review_Hawaii_clean_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_Hawaii_feature = review_Hawaii_clean_eng[['user_id', 'gmap_id', 'text', 'rating']]\n",
    "review_Hawaii_feature.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the review_Hawaii_feature into train (80%) and test (20%) datasets\n",
    "trainData, testData = train_test_split(review_Hawaii_feature, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optionally, you can reset indices of both DataFrames if needed\n",
    "trainData.reset_index(drop=True, inplace=True)\n",
    "testData.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. sim - user_id v.s. gmap_id predict rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set) # Maps an item to the users who rated it\n",
    "itemsPerUser = defaultdict(set) # Maps a user to the items that they rated\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "# itemNames = {}\n",
    "ratingDict = {} # To retrieve a rating for a specific user/item pair\n",
    "\n",
    "for _, d in trainData.iterrows():\n",
    "    user,item = d['user_id'], d['gmap_id']\n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)\n",
    "    reviewsPerUser[user].append(d)\n",
    "    reviewsPerItem[item].append(d)\n",
    "    ratingDict[(user,item)] = d['rating']\n",
    "    # itemNames[item] = d['product_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_true, y_pred):\n",
    "    differences = [(x-y)**2 for x,y in zip(y_true,y_pred)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine(i1, i2):\n",
    "    # Between two items\n",
    "    inter = usersPerItem[i1].intersection(usersPerItem[i2])\n",
    "    numer = 0\n",
    "    denom1 = 0\n",
    "    denom2 = 0\n",
    "    for u in inter:\n",
    "        numer += ratingDict[(u,i1)]*ratingDict[(u,i2)]\n",
    "    for u in usersPerItem[i1]:\n",
    "        denom1 += ratingDict[(u,i1)]**2\n",
    "    for u in usersPerItem[i2]:\n",
    "        denom2 += ratingDict[(u,i2)]**2\n",
    "    denom = math.sqrt(denom1) * math.sqrt(denom2)\n",
    "    if denom == 0: return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingMean = sum([d['rating'] for _, d in trainData.iterrows()]) / len(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "userAverages = {}\n",
    "itemAverages = {}\n",
    "\n",
    "for u in itemsPerUser:\n",
    "    rs = [ratingDict[(u,i)] for i in itemsPerUser[u]]\n",
    "    userAverages[u] = sum(rs) / len(rs)\n",
    "    \n",
    "for i in usersPerItem:\n",
    "    rs = [ratingDict[(u,i)] for u in usersPerItem[i]]\n",
    "    itemAverages[i] = sum(rs) / len(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Using user-user similarity with Jaccard predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRating_Jaccard(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['gmap_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'] - itemAverages[i2])\n",
    "        similarities.append(Jaccard(usersPerItem[item],usersPerItem[i2]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        ratingPrediction = itemAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "        return max(1, min(5, ratingPrediction))\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "simPredictions = [predictRating_Jaccard(d['user_id'], d['gmap_id']) for _, d in testData.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_rating = [d['rating'] for _, d in testData.iterrows()]\n",
    "mse_jaccard_user = MSE(simPredictions, true_rating)\n",
    "\n",
    "print(\"Jaccard User-based MSE: \", mse_jaccard_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Using user-user similarity with Cosine predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on user-user similarity with Cosine\n",
    "def predictRating_cos(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['gmap_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'] - itemAverages[i2])\n",
    "        similarities.append(Cosine(item,i2))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        ratingPrediction = itemAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "        return max(1, min(5, ratingPrediction))\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "simPredictions = [predictRating_cos(d['user_id'], d['gmap_id']) for _, d in testData.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_rating = [d['rating'] for _, d in testData.iterrows()]\n",
    "mse_consin_user = MSE(simPredictions, true_rating)\n",
    "\n",
    "print(\"Cosine User-based MSE: \", mse_consin_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Using user-user similarity with Jaccard predict with weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on user-user similarity Jaccard with weight\n",
    "def predictRating_Jaccard_weight(user,item, weight):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['gmap_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'] - itemAverages[i2])\n",
    "        similarities.append(pow(Jaccard(usersPerItem[item],usersPerItem[i2]), weight))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        ratingPrediction = itemAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "        return max(1, min(5, ratingPrediction))\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "best_weight = None\n",
    "best_MSE = None\n",
    "for weight in weights:\n",
    "    simPredictions = [predictRating_Jaccard_weight(d['user_id'], d['gmap_id'], weight) for _, d in testData.iterrows()]\n",
    "    true_rating = [d['rating'] for _, d in testData.iterrows()]\n",
    "    print(f\"Weight: {weight}, MSE: {MSE(simPredictions, true_rating)}\")\n",
    "    if best_weight is None or MSE(simPredictions, true_rating) < best_MSE:\n",
    "        best_weight = weight\n",
    "        best_MSE = MSE(simPredictions, true_rating)\n",
    "\n",
    "print(f\"Best weight: {best_weight}\")\n",
    "\n",
    "mse_best_jaccard = best_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. text mining - text predict rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bag-of-words models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore capitalization and remove punctuation\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for d in review_Hawaii_feature['text']:\n",
    "    r = ''.join([c for c in d.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w not in stop_words:\n",
    "            wordCount[w] += 1\n",
    "\n",
    "len(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = []\n",
    "for w in wordCount:\n",
    "    count.append((wordCount[w],w))\n",
    "count.sort(reverse=True)\n",
    "words = [x[1] for x in count[:1000]]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### using MSE to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in review_Hawaii_feature['text']:\n",
    "    r = ''.join([c for c in d.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w not in stop_words:\n",
    "            wordCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [0] * len(words)\n",
    "    r = ''.join([c for c in datum.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w not in stop_words and w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "            \n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(d) for d in review_Hawaii_feature['text']]\n",
    "y = [d for d in review_Hawaii_feature['rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_bg_ridge = MSE(y_test, predictions)\n",
    "mse_bg_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Try different evaluation method(Confusion Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    if y[i] >=4:\n",
    "        y[i] = 'positive'\n",
    "    elif y[i] <=2:\n",
    "        y[i] = 'negative'\n",
    "    else:\n",
    "        y[i] = 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_test)\n",
    "correct = predictions == y_test\n",
    "np.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_metric(y_true, y_pred, classes):\n",
    "    total_correct = 0\n",
    "\n",
    "    for cls in classes:\n",
    "        TP = FP = FN = TN = 0  # Initialize counts for this class\n",
    "\n",
    "        for true, pred in zip(y_true, y_pred):\n",
    "            if true == cls and pred == cls:\n",
    "                TP += 1  # True Positive\n",
    "                total_correct += 1\n",
    "            elif true != cls and pred == cls:\n",
    "                FP += 1  # False Positive\n",
    "            elif true == cls and pred != cls:\n",
    "                FN += 1  # False Negative\n",
    "            elif true != cls and pred != cls:\n",
    "                TN += 1  # True Negative\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "        print(f\"Class '{cls}':\")\n",
    "        print(f\"  True Positives (TP): {TP}\")\n",
    "        print(f\"  False Positives (FP): {FP}\")\n",
    "        print(f\"  False Negatives (FN): {FN}\")\n",
    "        print(f\"  True Negatives (TN): {TN}\")\n",
    "        print(f\"  Precision: {precision:.2f}\")\n",
    "        print(f\"  Recall: {recall:.2f}\\n\")\n",
    "\n",
    "classes = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "# Call the function\n",
    "confusion_metric(y_test, predictions, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text, remove punctuation, upper case, stop words\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text= ''.join(c for c in text if c not in string.punctuation)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "review_Hawaii_feature['text'] = review_Hawaii_feature['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=2000)  # Convert text to bag-of-words representation\n",
    "X = vectorizer.fit_transform(review_Hawaii_feature[\"text\"])\n",
    "\n",
    "y = review_Hawaii_feature[\"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "y_pred = model.predict(X_test)\n",
    "mse_BOW_LinearRegression = MSE(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse_BOW_LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word weights from the model\n",
    "weights = model.coef_\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "word_weights = dict(zip(vocab, weights))\n",
    "\n",
    "# Display top 10 words with the highest weights\n",
    "sorted_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 words with highest weights:\")\n",
    "for word, weight in sorted_words[:10]:\n",
    "    print(f\"{word}: {weight:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 words with lowest weights:\")\n",
    "for word, weight in sorted_words[-10:]:\n",
    "    print(f\"{word}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF - LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = review_Hawaii_feature['text']\n",
    "y = review_Hawaii_feature['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, max_features=2000, stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svr = LinearSVC()\n",
    "linear_svr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = linear_svr.predict(X_train_tfidf)\n",
    "y_pred_test = linear_svr.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = MSE(y_train, y_pred_train)\n",
    "test_mse = MSE(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train MSE: {train_mse:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n",
    "\n",
    "mse_linearsvc_tfidf = test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF - Ridge Regression(Redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, analyzer='word', max_features=2000, tokenizer=word_tokenize, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MR_train_X_tfidf, MR_test_X_tfidf, MR_train_Y_tfidf, MR_test_Y_tfidf = train_test_split(X, y, test_size=0.2, random_state=200)\n",
    "# MR_train_X_tfidf = MR_train_X_tfidf\n",
    "# MR_test_X_tfidf = MR_test_X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "MR_train_vector_tfidf = tfidf.fit_transform(MR_train_X_tfidf).toarray()\n",
    "MR_test_vector_tfidf = tfidf.transform(MR_test_X_tfidf).toarray()\n",
    "vocabulary_tfidf_train = tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "MR_test_vector_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.1, 0.5, 1 , 5, 10, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_MSE = None\n",
    "best_alpha = None\n",
    "for a in alpha:\n",
    "    clf = linear_model.Ridge(a, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(MR_train_vector_tfidf, MR_train_Y_tfidf)\n",
    "    predictions_tfidf = clf.predict(MR_test_vector_tfidf).clip(1, 5)\n",
    "    mse = MSE(MR_test_Y_tfidf, predictions_tfidf)\n",
    "    print(f\"Alpha: {a}, MSE: {mse}\")\n",
    "    if best_alpha is None or mse < best_MSE:\n",
    "        best_alpha = a\n",
    "        best_MSE = mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(MR_train_vector_tfidf, MR_train_Y_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictions = linear_model.predict(MR_test_vector_tfidf).clip(1, 5)\n",
    "mse_linear_reg_tfidf = MSE(MR_test_Y_tfidf, linear_predictions)\n",
    "\n",
    "print(f\"Linear Regression MSE: {mse_linear_reg_tfidf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF - SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_model = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "sgd_model.fit(MR_train_vector_tfidf, MR_train_Y_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model_predictions = sgd_model.predict(MR_test_vector_tfidf).clip(1, 5)\n",
    "mse_sgd_tfidf = MSE(MR_test_Y_tfidf, sgd_model_predictions)\n",
    "\n",
    "print(f\"SGD Regression MSE: {mse_sgd_tfidf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Latent Factor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise import SVDpp\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import BaselineOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(review_Hawaii_feature[['user_id', 'gmap_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineOnly()\n",
    "baseline_model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predictions = baseline_model.test(testset)\n",
    "mse_baseline_LF = MSE([d.r_ui for d in baseline_predictions], [d.est for d in baseline_predictions])\n",
    "\n",
    "print(f\"Baseline MSE: {mse_baseline_LF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_factors': [20, 50, 100],\n",
    "    'lr_all': [0.01, 0.1, 0.5],\n",
    "    'reg_all': [0.05, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(SVDpp, param_grid, measures=['rmse'], cv=3, n_jobs=-1, joblib_verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVD++ using optimal parameters\n",
    "best_params = gs.best_params['rmse']\n",
    "model = SVDpp(n_factors=best_params['n_factors'], lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.test(testset)\n",
    "mse_best_SVDpp = MSE([p.r_ui for p in predictions], [p.est for p in predictions])\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Test MSE: {mse_best_SVDpp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = review_Hawaii_feature['text']\n",
    "y = review_Hawaii_feature['rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_RFR = MSE(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
